{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88371f4a-e9f6-4fc8-8305-4a6a428f4e39",
   "metadata": {},
   "source": [
    "# 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb7565-a163-41af-879f-eef4fab1df0e",
   "metadata": {},
   "source": [
    "## 3.1 Random Forest Modeling Approach\n",
    "\n",
    "### Objective:\n",
    "Random Forest was selected as the initial model to establish a strong baseline due to its robustness, ability to handle mixed data types, and good out-of-the-box performance without heavy hyperparameter tuning.\n",
    "\n",
    "### Data Preprocessing:\n",
    "\n",
    "| Step                        | Details                                                                                                  |\n",
    "|------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| **Definition of Default**    | Loans labeled as 1 if `loan_status` was 'Charged Off' or 'Default' ; otherwise labeled as 0.               |\n",
    "| **Preventing Data Leakage**  | Post-loan variables (`recoveries`, `total_pymnt`, `last_pymnt_d`, etc.) were removed to ensure only pre-loan information was used for training..                    |\n",
    "| **Handling Missing Values**  | Rows with missing values were dropped to maintain dataset consistency and avoid biases from imputation.                                  |\n",
    "| **Feature Engineering**      | - Converted `earliest_cr_line` to datetime format. <br> - Created a new feature `credit_history_years` to represent the borrower’s credit history length. <br> - Original `earliest_cr_line` column was dropped. |\n",
    "| **Categorical Encoding**     | One-hot encoding on categorical features (`home_ownership`, `grade`, `emp_length`, `purpose`, `verification_status`, `term`) with `drop_first=True`. |\n",
    "\n",
    "### Model Variations:\n",
    "\n",
    "However, loan default is a rare event in our dataset, so a baseline Random Forest tended to predict almost all loans as non-defaults—achieving high overall accuracy but missing most actual defaults. To correct this imbalance we applied two techniques:\n",
    "\n",
    "1. Class Weighting: \n",
    "\n",
    "We increased the penalty for misclassifying defaults during training by assigning higher weight to the default class (and lower weight to non-defaults). This forces the Random Forest to pay more attention to the minority class and slightly boosts default detection.\n",
    "\n",
    "2. Threshold Tuning\n",
    "\n",
    "Beyond altering the training loss, we also adjusted the probability cutoff used to declare a loan “default.” Instead of the standard 0.5 threshold, we lowered it (to 0.2) for both the baseline and the class-weighted models. This shift trades off some false positives (flagging safe loans) for a much higher true positive rate—crucial when undetected defaults carry greater financial risk than a few extra false alarms.\n",
    "\n",
    "By adjusting the classification cutoff from the default value of 0.5, the model can prioritise recall over precision — a necessary trade-off given the investment strategy's goal of minimising undetected defaults. \n",
    "\n",
    "| Model Variation                            | Description                                                                                                                                               |\n",
    "|---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Baseline Random Forest**                  | Random Forest trained with default settings, no adjustments to threshold or class balance.                                                               |\n",
    "| **Threshold-Tuned Random Forest**           | Lowered decision threshold to 0.2 to prioritise identifying defaults, increasing recall at the cost of more false positives.                             |\n",
    "| **Class-Weighted Random Forest**            | Applied higher weight to defaults (2.41) and lower weight to non-defaults (0.63) during model training to address class imbalance.                        |\n",
    "| **Threshold-Tuned Class-Weighted Random Forest** | Combined class weighting with threshold adjustment (threshold 0.2) to maximise recall while controlling the precision-recall trade-off.                |\n",
    "\n",
    "### Evaluation and Decision:\n",
    "\n",
    "| Model                     | Recall (Default) | Precision (Default) | Recall (Non-Default) | Precision (Non-Default) | AUC   |\n",
    "|---------------------------|------------------|----------------------|----------------------|-------------------------|-------|\n",
    "| Baseline                  | 0.17             | 0.58                 | 0.97                 | 0.82                    | 0.776 |\n",
    "| Threshold-Tuned Baseline  | 0.81             | 0.34                 | 0.59                 | 0.92                    | 0.776 |\n",
    "| Class-Weighted            | 0.15             | 0.59                 | 0.97                 | 0.81                    | 0.779 |\n",
    "| Threshold-Tuned Weighted  | 0.79             | 0.35                 | 0.62                 | 0.92                    | 0.779 |\n",
    "\n",
    "Although threshold tuning significantly improved recall, the general performance of Random Forest models plateaued. Their ability to distinguish defaults remained limited, motivating exploration of more advanced ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b7004-a463-420d-83d8-bc265107d97b",
   "metadata": {},
   "source": [
    "## 3.2 XGBoost Modeling Approach\n",
    "\n",
    "### Objective:\n",
    "XGBoost was implemented following Random Forest to achieve higher recall, better handling of class imbalance, and improved scalability.\n",
    "\n",
    "### Data Preprocessing:\n",
    "\n",
    "Included some additional preprocessing not explored in Random Forest.\n",
    "\n",
    "| Step                        | Details                                                                                                  |\n",
    "|------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| **Definition of Default**    | Loans labeled as 1 if `loan_status` was 'Charged Off' or 'Default' ; otherwise labeled as 0.               |\n",
    "| **Preventing Data Leakage**  | Post-loan variables were excluded.                                                                      |\n",
    "| **Handling Missing Values**  | - Continuous features imputed with the median. <br> - Categorical features imputed with \"Unknown\".        |\n",
    "| **Target Encoding**          | Defaults mapped to 1; others mapped to 0.                                                                |\n",
    "| **Feature Engineering**      | Created `cr_hist` by computing the difference in months between the loan issue date `issue_d` and the borrower's earliest credit line `earliest_cr_line`.                           |\n",
    "| **Categorical Encoding**     | One-hot encoding applied, dropping the first category to avoid multicollinearity.                        |\n",
    "| **Type Consistency & Cleaning** | Columns were cleaned, continuous features enforced as floats, and stratified sampling was used for train-test split. |\n",
    "\n",
    "\n",
    "### Model Variations:\n",
    "\n",
    "| Model Variation                       | Description                                                                                                                                      |\n",
    "|----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Baseline XGBoost**                   | Trained with default parameters and no class weighting; used as a starting benchmark.                                                            |\n",
    "| **Weighted XGBoost**                   | Introduced `scale_pos_weight` to adjust for class imbalance by giving more importance to the minority (default) class during training.           |\n",
    "| **Weighted and Tuned XGBoost**          | Applied class weighting and tuned hyperparameters (e.g., learning rate, depth, subsample) to optimise performance across AUC, precision, and recall.|\n",
    "\n",
    "### Evaluation and Final Model Selection:\n",
    "\n",
    "#### Threshold Tuning\n",
    "\n",
    "Similar to the approach used in Random Forest, threshold tuning was applied to the class-weighted and hyperparameter-optimised XGBoost model to prioritise the minimisation of undetected defaults. The model's performance across different threshold levels is summarised below:\n",
    "\n",
    "| Threshold | Precision | Recall  | F1 Score | ROC AUC |\n",
    "|-----------|-----------|---------|----------|---------|\n",
    "| 0.30      | 0.2626    | 0.9256  | 0.4091   | 0.7317  |\n",
    "| 0.50      | 0.3463    | 0.6815  | 0.4592   | 0.7317  |\n",
    "| 0.70      | 0.4886    | 0.2862  | 0.3610   | 0.7317  |\n",
    "\n",
    "At a threshold of 0.30, the model achieves a recall of 92.56%, significantly reducing the likelihood of missing high-risk borrowers. Compared to the Random Forest models, this evaluation demonstrates superior recall and overall predictive performance, making XGBoost a more effective tool for minimising default risk.\n",
    "\n",
    "#### Final Model Selection\n",
    "\n",
    "Based on this evaluation, the **Class-Weighted and Tuned XGBoost** model with a **threshold of 0.30** was selected as the final model. It achieved the strongest balance between recall, F1 score, and overall model robustness, aligning best with the strategic objective of minimising default risk in the investment portfolio.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
