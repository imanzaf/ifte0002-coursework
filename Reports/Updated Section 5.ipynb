{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcde924-4ed4-4035-bc14-914654e74dec",
   "metadata": {},
   "source": [
    "# **5. Experiments and Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab146a-0c5e-4af3-a8ab-ff6e241c3968",
   "metadata": {},
   "source": [
    "# 5.1 Random Forest Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73e02f-4c2f-4547-977b-1ae23da87eb1",
   "metadata": {},
   "source": [
    "To establish a benchmark, we first trained and evaluated several variations of Random Forest models. Given the imbalanced nature of the dataset, we experimented with both threshold tuning and class weighting strategies.\n",
    "\n",
    "| **Model**                  | **Recall (Default)** | **Precision (Default)** | **Recall (Non-Default)** | **Precision (Non-Default)** | **AUC** |\n",
    "|-----------------------------|----------------------|--------------------------|---------------------------|------------------------------|---------|\n",
    "| Baseline                    | 0.17                 | 0.58                     | 0.97                      | 0.82                         | 0.776   |\n",
    "| Threshold-Tuned Baseline    | **0.81**             | 0.34                     | 0.59                      | 0.92                         | 0.776   |\n",
    "| Class-Weighted              | 0.15                 | 0.59                     | 0.97                      | 0.81                         | 0.779   |\n",
    "| Threshold-Tuned Weighted    | **0.79**             | 0.35                     | 0.62                      | 0.92                         | 0.779   |\n",
    "\n",
    "\n",
    "Initially, the baseline model showed strong ability to identify non-default loans but struggled to capture defaults. Applying threshold tuning—lowering the classification threshold to 0.2—substantially improved default recall (from 0.17 to 0.81), though at the cost of reduced precision.\n",
    "\n",
    "Class weighting was also applied by assigning computed weights (approximately 2.41 for defaults and 0.63 for non-defaults), but weighting alone did not meaningfully boost recall. Threshold tuning again proved to have the greatest impact.\n",
    "\n",
    "We selected the **threshold-tuned baseline Random Forest** as the final version among Random Forest models due to its slightly better recall. However, limitations remained, including an increased rate of false positives and the model's lack of transparency.\n",
    "\n",
    "Given these constraints, we moved toward exploring **XGBoost**, a more advanced modeling approach, to further improve default prediction performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767019e6-bf14-4323-ac36-ea295f1affb4",
   "metadata": {},
   "source": [
    "# 5.2 XGBoost Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef7654-a14f-454c-9891-f1f0d17f504e",
   "metadata": {},
   "source": [
    "Building on lessons from Random Forest, we transitioned to **XGBoost**, a gradient boosting framework designed to deliver stronger predictive performance for structured data.\n",
    "\n",
    "### Model Variants Compared:\n",
    "- **Baseline XGBoost** (no class weighting, default hyperparameters)\n",
    "- **Class-Weighted XGBoost** (using `scale_pos_weight`)\n",
    "- **Class-Weighted and Tuned XGBoost** (hyperparameter tuning + class weighting)\n",
    "\n",
    "The final selected model was the **Class-Weighted and Tuned XGBoost**, achieving:\n",
    "- **Accuracy**: 0.6607\n",
    "- **F1 Score**: 0.4592\n",
    "- **ROC AUC**: 0.7317\n",
    "\n",
    "This model best balanced precision, recall, and AUC.\n",
    "\n",
    "\n",
    "### Overfitting Assessment\n",
    "\n",
    "To evaluate model stability, ROC AUC scores were compared across training, test, and cross-validation sets:\n",
    "\n",
    "| Model                     | Train AUC | Test AUC | 3-Fold CV AUC (Train) |\n",
    "|----------------------------|-----------|----------|-----------------------|\n",
    "| Baseline XGBoost           | 0.7285    | 0.7278   | 0.7262 ± 0.0010        |\n",
    "| Tuned XGBoost (final)      | 0.7444    | 0.7317   | 0.7295 ± 0.0011        |\n",
    "\n",
    "Both models showed minimal overfitting, with consistent performance between training and testing.\n",
    "\n",
    "\n",
    "### Learning Curve Analysis\n",
    "\n",
    "- **Training AUC** started high (~0.84) and gradually declined as more data was introduced, converging toward ~0.75.\n",
    "- **Validation AUC** steadily increased from ~0.718 to ~0.729.\n",
    "- The narrowing gap between training and validation curves indicated moderate but manageable overfitting.\n",
    "\n",
    "This suggested that the model generalizes well, and additional data only offers diminishing returns after a point.\n",
    "\n",
    "\n",
    "### Threshold Tuning: Precision–Recall Trade-Off\n",
    "\n",
    "Since minimizing missed defaults was prioritized, recall was emphasized during threshold tuning. Three thresholds were evaluated:\n",
    "\n",
    "| Threshold | Precision | Recall | F1 Score | ROC AUC | KS Statistic |\n",
    "|-----------|-----------|--------|----------|---------|--------------|\n",
    "| 0.30      | 0.2626    | 0.9256 | 0.4091   | 0.7317  | 0.3373       |\n",
    "| 0.50      | 0.3463    | 0.6815 | 0.4592   | 0.7317  | 0.3373       |\n",
    "| 0.70      | 0.4886    | 0.2862 | 0.3610   | 0.7317  | 0.3373       |\n",
    "\n",
    "At a **threshold of 0.30**, recall reached **92.56%**, making it the most appropriate threshold for catching as many risky loans as possible, even if it increased false positives.\n",
    "\n",
    "Thus, the final XGBoost model was implemented with a **threshold of 0.30** to maximize default detection.\n",
    "\n",
    "### Feature Importance Analysis\n",
    "\n",
    "Using the built-in feature importance scores from the tuned XGBoost model, the top predictors of loan default were identified. These values reflect the relative contribution of each feature to the model's predictive performance (based on split gain/importance).\n",
    "\n",
    "**Top Predictive Features (from XGBoost):**\n",
    "1. `int_rate` — Most important predictor: Higher interest rates are strongly associated with higher credit risk and default probability, as they often reflect a lender's assessment of borrower risk.\n",
    "2. `grade_B` — These represent borrower credit grades and are inherently tied to loan risk profiling, underwriting decisions, and expected repayment behavior.\n",
    "3. `term_60 months` — Longer-term loans may carry higher risk due to extended exposure and borrower volatility over time.\n",
    "4. `home_ownership_MORTGAGE` — Homeownership status can be a proxy for financial stability. Borrowers who rent or carry mortgages may be at higher risk compared to outright homeowners.\n",
    "5. `emp_length_Unknown` — Missing employment length data could signal weaker credit profiles or incomplete applications, which may correspond with elevated risk.\n",
    "6. `fico_range_high` — As expected, credit score features were important in assessing risk, with lower FICO ranges linked to higher default likelihood.\n",
    "7. `dti`,`loan_amnt` — Larger loan amounts and higher DTI ratios are associated with repayment challenges and credit strain.\n",
    "8. `verification_status_Verified` — Loans with income verification appear to influence default probability, either through improved risk assessment or signaling borrower transparency.\n",
    "9. `purpose_small_business` — Loans for small business purposes tend to carry higher risk due to the volatility and failure rates of SMEs.\n",
    "10. `installment` — Monthly repayment amounts reflect loan structure and affordability dynamics, which tie directly to repayment behavior.\n",
    "\n",
    "Note: Other `grade` features not mentioned e.g `grade_C`, `grade_D`, etc, have the same interpretation as `grade_B`, albeit with lower importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346250f-6efc-46c1-a373-0afd694cc343",
   "metadata": {},
   "source": [
    "# 5.3 Detailed Comparative Analysis of ANN, XGBoost, and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2ca06-05fd-42ac-a232-96d7c1550368",
   "metadata": {},
   "source": [
    "### Table of Model Evaluation Metric Comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271fb85-daf9-412a-bc4e-f199e9616dde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "| Model                   | Accuracy | Precision (Class 1) | Recall (Class 1) | F1 Score (Class 1) | ROC AUC |\n",
    "|--------------------------|----------|---------------------|------------------|--------------------|---------|\n",
    "| ANN (Thresh 0.45)         | 0.6680   | 0.3660              | 0.7760           | 0.4980             | 0.7750  |\n",
    "| XGB (Thresh 0.30)         | 0.6607   | 0.2626              | 0.9256           | 0.4091             | 0.7317  |\n",
    "| XGB (Thresh 0.50)         | 0.6607   | 0.3463              | 0.6815           | 0.4592             | 0.7317  |\n",
    "| XGB (Thresh 0.70)         | 0.6607   | 0.4886              | 0.2862           | 0.3610             | 0.7317  |\n",
    "| RF (Baseline)             | 0.8030   | 0.5800              | 0.1700           | 0.2600             | 0.7760  |\n",
    "| RF (Class Weighted)       | 0.8020   | 0.5900              | 0.1500           | 0.2400             | 0.7790  |\n",
    "| RF (Thresh 0.2)           | 0.8020   | 0.3400              | 0.8100           | 0.4800             | 0.7790  |\n",
    "| RF Weighted + Thresh 0.2  | 0.8020   | 0.3500              | 0.7900           | 0.4900             | 0.7790  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737373f4-863b-4a94-b7fd-4cb34a4638a7",
   "metadata": {},
   "source": [
    "### ANN\n",
    "\n",
    "**Strengths:**\n",
    "- Strong recall (0.776), which is vital in high-risk settings (e.g., catching defaulters).\n",
    "- Balanced F1 score (0.498), meaning a good trade-off between precision and recall.\n",
    "- High ROC AUC (0.775), indicating strong overall discriminatory power.\n",
    "\n",
    "**Limitations:**\n",
    "- Lower precision (0.366) compared to threshold-tuned Random Forest (0.59) or XGBoost at 0.7 (0.4886), meaning more false positives.\n",
    "- Lower accuracy (0.668) compared to Random Forest models (0.802–0.803).\n",
    "\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "**Insights:**\n",
    "- At threshold 0.30, XGBoost delivers the highest recall (92.56%), but sacrifices precision heavily — useful when missing a defaulter is more costly than a false alarm.\n",
    "- At threshold 0.50, it achieves the best balance between recall and precision, yielding a competitive F1 score (0.4592).\n",
    "- At threshold 0.70, XGBoost becomes precision-focused, identifying fewer positives but with high accuracy — suited to false-positive-sensitive applications.\n",
    "\n",
    "**Overall:**\n",
    "- Flexible model when paired with threshold tuning.\n",
    "- Slightly lower AUC than ANN and Random Forest, suggesting marginally weaker ranking performance.\n",
    "\n",
    "\n",
    "### Random Forest (RF)\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Baseline RF is biased toward the majority class (class 0), with poor recall (0.17).\n",
    "- Class weighting alone does not significantly improve recall, indicating that rebalancing class importance isn't sufficient on its own.\n",
    "- Threshold tuning drastically improves recall to 0.81, and F1 to 0.48.\n",
    "- The best RF configuration is class weighting + threshold tuning, achieving the highest F1 score (0.49) among all models and competitive recall and precision.\n",
    "\n",
    "**AUC**: Highest among all models (tied with ANN), confirming strong overall discrimination ability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb1afb-2102-4c9d-aa01-912fd10e0e39",
   "metadata": {},
   "source": [
    "### Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05797978-2416-4803-b7c0-f6fb7cd69acf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "To meet the coursework objective of minimising default**, XGBoost (threshold 0.30) is the most suitable primary model. However, to ensure operational balance and reduce unnecessary loan rejections, it may be advisable to pair it with a Random Forest model in a dual-stage system, or use business-defined thresholds to modulate risk appetite over time.\n",
    "\n",
    "This approach not only optimises for technical accuracy, but also aligns with the practical and regulatory demands of credit risk management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e68c1-3b21-4252-9407-9f2947e7d3dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "| Goal                          | Best Model                  | Reasoning                                            |\n",
    "|------------------------------- |----------------------------- |----------------------------------------------------- |\n",
    "| Maximizing Recall              | XGBoost @ Threshold 0.30     | Catching nearly all positives (recall = 0.93)        |\n",
    "| Maximizing Precision           | XGBoost @ Threshold 0.70     | Highest precision (0.49) with controlled recall      |\n",
    "| Balanced F1 (Fair Trade-off)    | RF Weighted + Threshold      | Best F1 (0.49), strong recall and decent precision   |\n",
    "| Overall AUC Performance        | ANN or RF (any tuned)         | AUC (0.775-0.779), best class discrimination         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2612bfe-cca3-442e-a671-21fe3a56cc8a",
   "metadata": {},
   "source": [
    "# 5.4 Low-Risk Loan Portfolio Analysis: XGBoost and ANN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d528f-0454-4f81-a916-650c191948b1",
   "metadata": {},
   "source": [
    "To segment the loan portfolio into low-risk loans, different strategies based on predicted default probability thresholds were employed. This allowed the creation of portfolios with varying risk levels, enabling a tailored strategy for managing loan defaults.\n",
    "\n",
    "## XGBoost Model Analysis:\n",
    "\n",
    "**Threshold-Based Portfolio Selection (< 0.2):**\n",
    "- **Total Loans Selected**: 19,277  \n",
    "- **Observed Default Rate**: 3.94%  \n",
    "\n",
    "Loans in this portfolio are the least likely to default, with a very low predicted default probability. This portfolio has a significantly lower default rate than the overall dataset, making it ideal for minimising risk.\n",
    "\n",
    "\n",
    "**Top N Loan Portfolio (Top 300 Loans)**\n",
    "- **Total Loans Selected**: 300  \n",
    "- **Observed Default Rate**: 1.33%  \n",
    "\n",
    "By selecting the top 300 loans with the lowest predicted default probabilities, we achieve an even lower default rate (1.33%), reflecting a high concentration of quality loans. However, the number of loans is small, which may limit portfolio size and diversification.\n",
    "\n",
    "\n",
    "**Top X% Loan Portfolio (Top 10% Loans)**\n",
    "- **Total Loans Selected**: 15,134  \n",
    "- **Observed Default Rate**: 3.36%  \n",
    "\n",
    "This approach selects the top 10% safest loans based on predicted probabilities. It allows for a larger pool of loans with a relatively low default rate, though slightly higher than the stricter threshold of 0.2.\n",
    "\n",
    "\n",
    "## ANN Model Analysis (Threshold < 0.45):\n",
    "\n",
    "**Overall Low-Risk Loan Portfolio:**\n",
    "- **Total Loans Selected:** 33,325\n",
    "- **Observed Default Rate:** 19.67%\n",
    " \n",
    "Using a threshold of 0.45 in the ANN model results in a larger selected portfolio but a relatively higher default rate compared to XGBoost’s stricter thresholding.\n",
    "\n",
    "**Top 300 Loans:**\n",
    "- **Observed Default Rate:** 10.00%\n",
    "- **Average ROI:** 12.45%\n",
    "\n",
    "**Top 500 Loans:**\n",
    "- **Observed Default Rate:** 10.87%\n",
    "- **Average ROI:** 11.60%\n",
    "\n",
    "**Top 1000 Loans:**\n",
    "- **Observed Default Rate:** 14.29%\n",
    "- **Average ROI:** 10.70%\n",
    " \n",
    "The ANN model identifies portfolios that maintain relatively strong return on investment (ROI) alongside moderate control of default risk. However, even in the best-performing top segments, default rates are notably higher compared to XGBoost's best selections.\n",
    "\n",
    "## Strategic Insights\n",
    "\n",
    "| Strategy                                | Risk Level | Portfolio Size | Diversification | Potential ROI       |\n",
    "|-----------------------------------------|------------|----------------|-----------------|---------------------|\n",
    "| XGBoost Threshold < 0.2                 | Very Low   | Moderate       | Limited         | Not directly measured |\n",
    "| XGBoost Top 300 Loans                   | Ultra Low  | Very Small     | Very Limited    | Not directly measured |\n",
    "| XGBoost Top 10%                         | Low        | Large          | Good            | Not directly measured |\n",
    "| ANN Top 300 Loans                       | Moderate   | Small          | Limited         | High (12.45%)        |\n",
    "| ANN Top 500–1000 Loans                  | Moderate   | Moderate       | Moderate        | High (10.7%–11.6%)   |\n",
    "\n",
    "## Final Recommendations\n",
    "- For clients prioritizing ultra-low risk, XGBoost portfolios (especially Threshold < 0.2 or Top 300 selections) are preferable.\n",
    "- For those balancing risk and returns, ANN portfolios offer higher ROIs at the cost of a moderately higher default rate.\n",
    "- Top 10% XGBoost portfolios present a strong middle ground, offering both diversification and significantly reduced default risks compared to the general loan pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3e59a-744d-492a-ad31-85fa3e41a079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
