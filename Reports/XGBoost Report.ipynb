{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddae2855-784d-4a12-b7f0-3e576887e9c9",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Considerations and Rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b154050-1478-4cb6-b784-25863fdc6e6b",
   "metadata": {},
   "source": [
    "A number of structured steps were taken to ensure that the dataset was suitable for supervised machine learning, while minimising risks of bias, data leakage, or inconsistency. Key considerations included:\n",
    "\n",
    "#### 1. Prevention of Data Leakage\n",
    "To ensure predictive integrity, all variables that reflect post-loan outcomes (such as `total_pymnt`, `recoveries`, or `last_pymnt_d`) were removed. These values are only known after the loan’s performance is observed, and thus would result in data leakage if included during training. This step is essential to ensure that model evaluation reflects realistic, out-of-sample performance.\n",
    "\n",
    "#### 2. Handling Missing Values\n",
    "Two different imputation strategies were used based on variable type:\n",
    "- For **continuous features**, missing values were filled with the median, preserving distribution without introducing artificial skew.\n",
    "- For **categorical features**, missing values were imputed using a placeholder label \"Unknown\" to retain observations while making missingness explicit.\n",
    "\n",
    "This avoided unnecessary row deletion while ensuring compatibility with model training.\n",
    "\n",
    "#### 3. Target Variable Encoding\n",
    "The `loan_status` field was mapped into a binary target variable (`target`) to distinguish default events:\n",
    "- Loans marked as `\"Charged Off\"` or `\"Default\"` were assigned a label of 1 (default).\n",
    "- All others were treated as non-default (0).\n",
    "\n",
    "This transformation enabled the application of binary classification techniques.\n",
    "\n",
    "#### 4. Temporal Feature Engineering\n",
    "To capture borrower credit history, a new feature `cr_hist` was created by computing the difference in months between the loan issue date (`issue_d`) and the borrower's earliest credit line (`earliest_cr_line`).\n",
    "\n",
    "All date variables were converted to `datetime` format to allow reliable manipulation.\n",
    "\n",
    "#### 5. Categorical Encoding\n",
    "All categorical variables were encoded using one-hot encoding with `drop_first=True` to avoid multicollinearity. This ensured compatibility with tree-based models, while preserving interpretability of categorical inputs.\n",
    "\n",
    "#### 6. Type Consistency and Clean-Up\n",
    "To avoid errors during model fitting (especially with XGBoost), careful steps were taken to:\n",
    "- Enforce all continuous features to be of float dtype.\n",
    "- Flatten any nested arrays or DataFrames in the target variable.\n",
    "- Remove illegal characters from column names (`[`, `]`, `<`, `>`), which are unsupported by certain model APIs.\n",
    "- Check and verify that all columns were valid and accessible before fitting the model.\n",
    "\n",
    "This ensured seamless integration with `scikit-learn` and `XGBoost` pipelines.\n",
    "\n",
    "#### 7. Train-Test Splitting with Stratification\n",
    "The data was split into training and test sets using a stratified split on the target variable. This preserved the proportion of defaulters in both sets, ensuring consistent model evaluation — particularly important in imbalanced classification problems such as loan default prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8155f-4cbc-4b85-b84e-4666a3f4fdf9",
   "metadata": {},
   "source": [
    "# Model Performance Comparison: Baseline vs Weighted vs Tuned XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2c14c-fea1-4738-8f6a-5bc6f872f907",
   "metadata": {},
   "source": [
    "To evaluate the impact of class weighting and hyperparameter tuning on model performance, three variants of the XGBoost classifier were developed and compared on the test set.\n",
    "\n",
    "\n",
    "#### Model 1: Baseline XGBoost (No Weights, No Tuning)\n",
    "This model used default XGBoost parameters without any adjustment for class imbalance.\n",
    "\n",
    "- **Accuracy**: 0.7947  \n",
    "- **F1 Score**: 0.2159  \n",
    "- **ROC AUC**: 0.7301  \n",
    "\n",
    "While the baseline model showed high accuracy, the F1 score was notably low, reflecting poor performance on the minority (default) class. This is typical in imbalanced datasets, where accuracy can be misleading due to the model's bias toward the majority class (non-default).\n",
    "\n",
    "\n",
    "#### Model 2: Weighted XGBoost (Class Weights, No Tuning)\n",
    "In this version, the `scale_pos_weight` parameter was set to the ratio of non-defaults to defaults to address class imbalance.\n",
    "\n",
    "- **Accuracy**: 0.6629  \n",
    "- **F1 Score**: 0.4566  \n",
    "- **ROC AUC**: 0.7288  \n",
    "\n",
    "Here, accuracy decreased (as expected) because the model no longer over-predicted the majority class. However, both the F1 score and recall improved significantly, indicating better detection of actual defaulters. This suggests that class weighting is essential in improving fairness and effectiveness when defaults are rare.\n",
    "\n",
    "\n",
    "#### Model 3: Tuned XGBoost (Class Weights + Hyperparameter Tuning)\n",
    "This final model combined class weighting with a tuned set of hyperparameters via grid search.\n",
    "\n",
    "- **Accuracy**: 0.6607  \n",
    "- **F1 Score**: 0.4592  \n",
    "- **ROC AUC**: 0.7317  \n",
    "\n",
    "Compared to Model 2, the tuned model achieved a slightly higher AUC and marginally improved F1 score, though overall gains were modest. This implies that while tuning helped refine model discrimination, class weighting remained the primary driver of performance improvement in terms of recall and F1.\n",
    "\n",
    "\n",
    "### Model Selected\n",
    "\n",
    "Based on the evaluation of all three models, Model 3: Tuned XGBoost (Class Weights + Hyperparameter Tuning) is the most appropriate choice for this loan default prediction task. While the performance gains over Model 2 were modest, this model achieved the highest F1 score (0.4592) and highest ROC AUC (0.7317), indicating the best overall balance between precision, recall, and discriminatory capability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b2252-a17f-4c42-8958-92035221f611",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3a90b-423c-4f37-b61d-f15a1f8f3c79",
   "metadata": {},
   "source": [
    "Using the built-in feature importance scores from the tuned XGBoost model, the top predictors of loan default were identified. These values reflect the relative contribution of each feature to the model's predictive performance (based on split gain/importance).\n",
    "\n",
    "### Top Predictive Features (from XGBoost):\n",
    "1. `int_rate` — Most important predictor: Higher interest rates are strongly associated with higher credit risk and default probability, as they often reflect a lender's assessment of borrower risk.\n",
    "2. `grade_B` — These represent borrower credit grades and are inherently tied to loan risk profiling, underwriting decisions, and expected repayment behavior.\n",
    "3. `term_60 months` — Longer-term loans may carry higher risk due to extended exposure and borrower volatility over time.\n",
    "4. `home_ownership_MORTGAGE` — Homeownership status can be a proxy for financial stability. Borrowers who rent or carry mortgages may be at higher risk compared to outright homeowners.\n",
    "5. `emp_length_Unknown` — Missing employment length data could signal weaker credit profiles or incomplete applications, which may correspond with elevated risk.\n",
    "6. `fico_range_high` — As expected, credit score features were important in assessing risk, with lower FICO ranges linked to higher default likelihood.\n",
    "7. `dti`,`loan_amnt` — Larger loan amounts and higher DTI ratios are associated with repayment challenges and credit strain.\n",
    "8. `verification_status_Verified` — Loans with income verification appear to influence default probability, either through improved risk assessment or signaling borrower transparency.\n",
    "9. `purpose_small_business` — Loans for small business purposes tend to carry higher risk due to the volatility and failure rates of SMEs.\n",
    "10. `installment` — Monthly repayment amounts reflect loan structure and affordability dynamics, which tie directly to repayment behavior.\n",
    "\n",
    "Note: Other `grade` features not mentioned e.g `grade_C`, `grade_D`, etc, have the same interpretation as `grade_B`, albeit with lower importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b02444-cea0-469f-8e1b-a4168c91d280",
   "metadata": {},
   "source": [
    "# Overfitting Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352eebd-eb7f-4e31-a6e7-5e041263de53",
   "metadata": {},
   "source": [
    "To evaluate the generalisability and stability of the XGBoost models, an overfitting analysis was conducted by comparing training, test, and cross-validation ROC AUC scores for both the baseline and tuned models.\n",
    "\n",
    "#### Baseline Model\n",
    "- **Train ROC AUC**: 0.7285  \n",
    "- **Test ROC AUC**: 0.7278  \n",
    "- **3-Fold CV ROC AUC (Train)**: 0.7262 ± 0.0010  \n",
    "\n",
    "The baseline model demonstrated consistent performance across training, test, and cross-validation sets, indicating minimal overfitting. The near-identical ROC AUC values suggest the model generalises well despite not applying class weighting or tuning.\n",
    "\n",
    "#### Tuned Model (Class Weights + Grid Search)\n",
    "- **Train ROC AUC**: 0.7444  \n",
    "- **Test ROC AUC**: 0.7317  \n",
    "- **3-Fold CV ROC AUC (Train)**: 0.7295 ± 0.0011  \n",
    "\n",
    "The tuned model achieved a slightly higher training AUC, with a moderate improvement in test performance. The difference between training and test scores (≈ 0.013) is small, suggesting the model benefits from tuning without significantly overfitting. The cross-validation score closely aligns with the test score, further supporting its robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8118e-8138-4347-b5e9-a60df8feee58",
   "metadata": {},
   "source": [
    "# Learning Curve Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156aa717-8a8c-4fa3-a293-ddf1a4422970",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The learning curve provides insights into how model performance evolves with increasing training data size. This is essential for diagnosing underfitting, overfitting, and assessing whether additional training data could improve generalisation.\n",
    "\n",
    "#### Observations:\n",
    "- Training AUC starts high (~0.84) but decreases steadily as more data is introduced, converging toward ~0.75. This is expected and indicates reduced overfitting with increased data complexity.\n",
    "- Validation AUC begins lower (~0.718) and increases gradually, plateauing around ~0.729 with more data.\n",
    "- The gap between training and validation AUC narrows with more data, but does not fully close — suggesting moderate overfitting, which is typical in high-capacity models like XGBoost.\n",
    "\n",
    "#### Interpretation:\n",
    "- The model benefits from more data, as validation AUC improves consistently up to the maximum sample size used.\n",
    "- The performance plateaus near the end, implying that beyond this point, adding more data may not yield significant gains without other changes (e.g. feature engineering or model regularisation).\n",
    "- The final gap between training and validation curves indicates the model captures real patterns but still slightly overfits — this is not necessarily problematic, but something to monitor in production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61000c65-f5a7-4e54-8438-2aabb4b5ee4a",
   "metadata": {},
   "source": [
    "# Threshold Tuning: Precision–Recall Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e5c3a-01fc-4dd8-8f0a-3f04981dbafc",
   "metadata": {},
   "source": [
    "While the model's optimal threshold based on F1 score was identified as 0.50, threshold adjustment can be used strategically to meet different operational goals — particularly in imbalanced classification problems like loan default prediction.\n",
    "\n",
    "#### Contextual Objective: Prioritising Recall\n",
    "\n",
    "In the lending context, catching as many defaulters as possible is critical for minimising portfolio risk. Therefore, recall (true positive rate) is prioritised over precision, as missing a defaulter (false negative) is generally more costly than flagging a non-defaulter (false positive).\n",
    "\n",
    "#### Threshold Comparison Summary\n",
    "\n",
    "| Threshold | Precision | Recall | F1 Score | ROC AUC | KS Statistic |\n",
    "|-----------|-----------|--------|----------|---------|--------------|\n",
    "| 0.30      | 0.2626    | 0.9256 | 0.4091   | 0.7317  | 0.3373       |\n",
    "| 0.50      | 0.3463    | 0.6815 | 0.4592   | 0.7317  | 0.3373       |\n",
    "| 0.70      | 0.4886    | 0.2862 | 0.3610   | 0.7317  | 0.3373       |\n",
    "\n",
    "#### Interpretation:\n",
    "- At threshold = 0.30, the model captures over 92% of actual defaults, making it ideal when recall is critical. However, this comes at the cost of lower precision (more false positives).\n",
    "- Threshold = 0.50 yields the highest F1 score, balancing precision and recall.\n",
    "- At threshold = 0.70, precision improves significantly, but recall drops sharply — making it riskier in contexts where missing defaulters is unacceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f797cd-5cb2-4ae4-afd9-b160966dd525",
   "metadata": {},
   "source": [
    "# Loan Portfolio Selection: Risk-Based Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a3f17-14c7-402e-af34-fdbcaad2e035",
   "metadata": {},
   "source": [
    "To segment the loan portfolio into low-risk loans, different strategies based on predicted default probability thresholds were employed. This allowed the creation of portfolios with varying risk levels, enabling a tailored strategy for managing loan defaults.\n",
    "\n",
    "#### Objective: Minimise Default Risk by Selecting Low-Risk Loans\n",
    "\n",
    "**Threshold-Based Portfolio Selection:**\n",
    "\n",
    "1. **Selected Low-Risk Loan Portfolio (Threshold < 0.2)**\n",
    "   - **Total Loans Selected**: 19,277  \n",
    "   - **Observed Default Rate**: 3.94%  \n",
    "\n",
    "Loans in this portfolio are the least likely to default, with a very low predicted default probability. This portfolio has a significantly lower default rate than the overall dataset, making it ideal for minimising risk.\n",
    "\n",
    "\n",
    "2. **Top N Loan Portfolio (Top 300 Loans)**\n",
    "   - **Total Loans Selected**: 300  \n",
    "   - **Observed Default Rate**: 1.33%  \n",
    "\n",
    "By selecting the top 300 loans with the lowest predicted default probabilities, we achieve an even lower default rate (1.33%), reflecting a high concentration of quality loans. However, the number of loans is small, which may limit portfolio size and diversification.\n",
    "\n",
    "\n",
    "3. **Top X% Loan Portfolio (Top 10% Loans)**\n",
    "   - **Total Loans Selected**: 15,134  \n",
    "   - **Observed Default Rate**: 3.36%  \n",
    "\n",
    "This approach selects the top 10% safest loans based on predicted probabilities. It allows for a larger pool of loans with a relatively low default rate, though slightly higher than the stricter threshold of 0.2.\n",
    "\n",
    "\n",
    "#### Insights and Strategic Considerations:\n",
    "- **Threshold < 0.2** provides the lowest risk but results in a smaller portfolio. It is ideal for clients seeking ultra-low-risk loans, though it sacrifices portfolio size.\n",
    "- **Top 300 loans** offer a more concentrated portfolio, achieving the lowest default rate but with limited volume and diversification.\n",
    "- **Top 10% selection** strikes a balance between scale and risk, creating a larger and more diversified portfolio with a default rate still well below the dataset average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae3574-08c0-4228-a573-f0d3f416d022",
   "metadata": {},
   "source": [
    "# LLM Used For:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140738a8-3778-40e0-84ce-1424dd297c6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Learning curve analysis  \n",
    "- Confusion matrix interpretation  \n",
    "- Threshold tuning explanation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
