{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6120cd21-23f8-4504-9a45-4e5cdce975ff",
   "metadata": {},
   "source": [
    "# **3. Methodology**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194af2a-0d59-49a8-997f-7a9fa7f241b0",
   "metadata": {},
   "source": [
    "## **3.1 Random Forest Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc253d-720e-451b-8ee7-1d57be43ee20",
   "metadata": {},
   "source": [
    "We first developed a **Random Forest classifier** as an initial model for predicting loan defaults. Random Forest was selected for its strong baseline performance, robustness to overfitting, and ability to handle large feature sets without heavy tuning.\n",
    "A loan was defined as a **default** if its status was `'Charged Off'` or `'Default'`. To prevent **data leakage**, variables reflecting post-loan outcomes (such as `loan_status`, `recoveries`, and pre-calculated return columns) were removed before modeling. Missing values were handled by dropping rows containing any null values, ensuring the model trained only on complete data without introducing imputation biases.\n",
    "The Random Forest was trained in four variations:\n",
    "- Baseline model\n",
    "- Baseline model with threshold tuning\n",
    "- Class-weighted model\n",
    "- Class-weighted model with threshold tuning\n",
    "\n",
    "The threshold-tuned baseline model achieved the highest recall (**0.81**) among the Random Forest variants. However, Random Forest was ultimately surpassed in performance by XGBoost, prompting a shift toward using more advanced boosting techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b87217-578f-4179-b3c7-90e6bbcd3486",
   "metadata": {},
   "source": [
    "## **3.2 XGBoost Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26bb93-fe83-48a9-bac1-a4f6edff878e",
   "metadata": {},
   "source": [
    "Building on the experience from Random Forest, we adopted a more structured preprocessing pipeline to optimize the dataset for **XGBoost**, a model known for its strong performance with tabular, structured data and its ability to manage class imbalance effectively.\n",
    "The following steps were taken:\n",
    "- **Prevention of Data Leakage**:\n",
    "All variables related to post-loan outcomes (e.g., `total_pymnt`, `recoveries`, `last_pymnt_d`) were removed. This ensured the model only used information available at loan origination.\n",
    "- **Handling Missing Values**:\n",
    "Missing values were imputed rather than dropped:\n",
    "    - **Continuous features** (e.g., `annual_inc`, `revol_util`) were filled using the median to preserve distribution robustness.\n",
    "    - **Categorical features** (e.g., `home_ownership`, `purpose`) were imputed with a new **'Unknown'** category, allowing retention of all observations while signaling missingness.\n",
    "- **Target Variable Encoding**:\n",
    "The loan_status variable was binarized, mapping `\"Charged Off\"` and `\"Default\"` loans to 1 (default), and all other statuses to 0 (non-default).\n",
    "- **Temporal Feature Engineering**:\n",
    "A new feature, `cr_hist`, representing borrower credit history length, was created by calculating the difference in months between `issue_d` (loan issuance date) and `earliest_cr_line` (first credit line).\n",
    "- **Categorical Encoding**:\n",
    "Categorical variables were encoded using **one-hot encoding** (`drop_first=True`) to prevent multicollinearity and ensure model compatibility.\n",
    "- **Type Consistency and Clean-Up**:\n",
    "Continuous features were cast to float types, and column names were standardized to prevent errors during XGBoost model fitting.\n",
    "- **Train-Test Splitting**:\n",
    "A **stratified train-test split** was performed to maintain the default rate proportion across the training and testing datasets, which is crucial given the class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c96e1b-1cc5-4294-af58-7ac9a99c739d",
   "metadata": {},
   "source": [
    "## **3.3 Modelling Experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c3a60-bf20-4a8e-9ae7-fb56d9d74d56",
   "metadata": {},
   "source": [
    "Three versions of XGBoost were developed and compared:\n",
    "- **Baseline XGBoost**: No class weighting, default parameters.\n",
    "- **Weighted XGBoost**: Using `scale_pos_weight` to address class imbalance.\n",
    "- **Weighted and Tuned XGBoost**: Further optimizing hyperparameters to refine model performance.\n",
    "The final selected model was the **class-weighted and tuned XGBoost**, which achieved the best trade-off between **precision**, **recall**, and **AUC**, outperforming Random Forest across all major evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00edc4cc-e8d8-4f14-9577-9f9a3c35b5d5",
   "metadata": {},
   "source": [
    "# **5. Experiments and Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92b003-0485-4980-9e2a-ac828dba2fa1",
   "metadata": {},
   "source": [
    "## **5.1 Random Forest Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141619e-6136-4391-aab5-d5c8a15fc8b0",
   "metadata": {},
   "source": [
    "To establish a benchmark, we first trained and evaluated several variations of Random Forest models. Given the imbalanced nature of the dataset, we experimented with both threshold tuning and class weighting strategies.\n",
    "\n",
    "| **Model**                  | **Recall (Default)** | **Precision (Default)** | **Recall (Non-Default)** | **Precision (Non-Default)** | **AUC** |\n",
    "|-----------------------------|----------------------|--------------------------|---------------------------|------------------------------|---------|\n",
    "| Baseline                    | 0.17                 | 0.58                     | 0.97                      | 0.82                         | 0.776   |\n",
    "| Threshold-Tuned Baseline    | **0.81**             | 0.34                     | 0.59                      | 0.92                         | 0.776   |\n",
    "| Class-Weighted              | 0.15                 | 0.59                     | 0.97                      | 0.81                         | 0.779   |\n",
    "| Threshold-Tuned Weighted    | **0.79**             | 0.35                     | 0.62                      | 0.92                         | 0.779   |\n",
    "\n",
    "Initially, the baseline model showed strong ability to identify non-default loans but struggled to capture defaults. Applying threshold tuning—lowering the classification threshold to 0.2—substantially improved default recall (from 0.17 to 0.81), though at the cost of reduced precision.\n",
    "\n",
    "Class weighting was also applied by assigning computed weights (approximately 2.41 for defaults and 0.63 for non-defaults), but weighting alone did not meaningfully boost recall. Threshold tuning again proved to have the greatest impact.\n",
    "\n",
    "We selected the **threshold-tuned baseline Random Forest** as the final version among Random Forest models due to its slightly better recall. However, limitations remained, including an increased rate of false positives and the model's lack of transparency.\n",
    "\n",
    "Given these constraints, we moved toward exploring **XGBoost**, a more advanced modeling approach, to further improve default prediction performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a69619-4dc6-4a00-9922-baf53973dce1",
   "metadata": {},
   "source": [
    "## **5.2 XGBoost Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a027d-87f0-4ed5-be93-44bf1c28b5af",
   "metadata": {},
   "source": [
    "Building on lessons from Random Forest, we transitioned to **XGBoost**, a gradient boosting framework designed to deliver stronger predictive performance for structured data.\n",
    "\n",
    "### Model Variants Compared:\n",
    "- **Baseline XGBoost** (no class weighting, default hyperparameters)\n",
    "- **Class-Weighted XGBoost** (using `scale_pos_weight`)\n",
    "- **Class-Weighted and Tuned XGBoost** (hyperparameter tuning + class weighting)\n",
    "\n",
    "The final selected model was the **Class-Weighted and Tuned XGBoost**, achieving:\n",
    "- **Accuracy**: 0.6607\n",
    "- **F1 Score**: 0.4592\n",
    "- **ROC AUC**: 0.7317\n",
    "\n",
    "This model best balanced precision, recall, and AUC.\n",
    "\n",
    "\n",
    "### Overfitting Assessment\n",
    "\n",
    "To evaluate model stability, ROC AUC scores were compared across training, test, and cross-validation sets:\n",
    "\n",
    "| Model                     | Train AUC | Test AUC | 3-Fold CV AUC (Train) |\n",
    "|----------------------------|-----------|----------|-----------------------|\n",
    "| Baseline XGBoost           | 0.7285    | 0.7278   | 0.7262 ± 0.0010        |\n",
    "| Tuned XGBoost (final)      | 0.7444    | 0.7317   | 0.7295 ± 0.0011        |\n",
    "\n",
    "Both models showed minimal overfitting, with consistent performance between training and testing.\n",
    "\n",
    "\n",
    "### Learning Curve Analysis\n",
    "\n",
    "- **Training AUC** started high (~0.84) and gradually declined as more data was introduced, converging toward ~0.75.\n",
    "- **Validation AUC** steadily increased from ~0.718 to ~0.729.\n",
    "- The narrowing gap between training and validation curves indicated moderate but manageable overfitting.\n",
    "\n",
    "This suggested that the model generalizes well, and additional data only offers diminishing returns after a point.\n",
    "\n",
    "\n",
    "### Threshold Tuning: Precision–Recall Trade-Off\n",
    "\n",
    "Since minimizing missed defaults was prioritized, recall was emphasized during threshold tuning. Three thresholds were evaluated:\n",
    "\n",
    "| Threshold | Precision | Recall | F1 Score | ROC AUC | KS Statistic |\n",
    "|-----------|-----------|--------|----------|---------|--------------|\n",
    "| 0.30      | 0.2626    | 0.9256 | 0.4091   | 0.7317  | 0.3373       |\n",
    "| 0.50      | 0.3463    | 0.6815 | 0.4592   | 0.7317  | 0.3373       |\n",
    "| 0.70      | 0.4886    | 0.2862 | 0.3610   | 0.7317  | 0.3373       |\n",
    "\n",
    "At a **threshold of 0.30**, recall reached **92.56%**, making it the most appropriate threshold for catching as many risky loans as possible, even if it increased false positives.\n",
    "\n",
    "Thus, the final XGBoost model was implemented with a **threshold of 0.30** to maximize default detection.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
