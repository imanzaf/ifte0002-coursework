{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88371f4a-e9f6-4fc8-8305-4a6a428f4e39",
   "metadata": {},
   "source": [
    "# 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb7565-a163-41af-879f-eef4fab1df0e",
   "metadata": {},
   "source": [
    "## 3.1 Random Forest Modeling Approach\n",
    "\n",
    "We initially developed a **Random Forest classifier** as a baseline model for predicting loan defaults. Random Forest was selected for its ability to handle mixed data types, robustness to noise, and effectiveness without heavy hyperparameter tuning.\n",
    "\n",
    "To ensure model validity:\n",
    "\n",
    "- **Definition of Default**: Loans were labeled as default (`1`) if their `loan_status` was `'Charged Off'` or `'Default'`; otherwise labeled non-default (`0`).\n",
    "- **Preventing Data Leakage**: Post-loan outcome variables (`recoveries`, `total_pymnt`, `last_pymnt_d`, etc.) were removed to ensure only pre-loan information was used for training.\n",
    "- **Handling Missing Values**: Rows with missing values were dropped to maintain dataset consistency and avoid biases from imputation.\n",
    "- **Feature Engineering**:\n",
    "  - `earliest_cr_line` was converted to datetime format.\n",
    "  - A new feature `credit_history_years` was created to represent the borrower’s credit history length.\n",
    "  - The original `earliest_cr_line` column was dropped.\n",
    "- **Categorical Encoding**: One-hot encoding was applied on known categorical columns (`home_ownership`, `grade`, `emp_length`, `purpose`, `verification_status`, `term`) with `drop_first=True`.\n",
    "\n",
    "We tested several variations:\n",
    "\n",
    "- **Baseline Random Forest** with default settings.\n",
    "- **Threshold-Tuned Random Forest**:\n",
    "Threshold tuning is a technique where the decision boundary of the classifier is adjusted. Instead of classifying an observation as \"default\" only if its predicted probability exceeds 0.5, the threshold is lowered to prioritize identifying defaults (even at the expense of more false positives).\n",
    "In this case, a threshold of 0.2 was selected to maximize recall for defaults.\n",
    "- **Class-Weighted Random Forest**, applying higher weight to defaults (2.41) and lower weight to non-defaults (0.63) during model training.\n",
    "- **Threshold-Tuned Class-Weighted Random Forest**, combining class weighting and threshold adjustment.\n",
    "\n",
    "\n",
    "### Random Forest Model Results\n",
    "\n",
    "| Model                     | Recall (Default) | Precision (Default) | Recall (Non-Default) | Precision (Non-Default) | AUC   |\n",
    "|---------------------------|------------------|----------------------|----------------------|-------------------------|-------|\n",
    "| Baseline                  | 0.17             | 0.58                 | 0.97                 | 0.82                    | 0.776 |\n",
    "| Threshold-Tuned Baseline  | 0.81             | 0.34                 | 0.59                 | 0.92                    | 0.776 |\n",
    "| Class-Weighted            | 0.15             | 0.59                 | 0.97                 | 0.81                    | 0.779 |\n",
    "| Threshold-Tuned Weighted  | 0.79             | 0.35                 | 0.62                 | 0.92                    | 0.779 |\n",
    "\n",
    "> Performance evaluation focused particularly on **recall for the default class**, given the business objective of minimizing undetected risky loans. Threshold tuning had the greatest impact, increasing recall from **0.17** to **0.81** for the baseline model at a threshold of **0.2**.\n",
    "Ultimately, the **Threshold-Tuned Baseline Random Forest** was selected as the best-performing Random Forest version. However, recognizing its limitations (e.g., increased false positives and lack of interpretability), a more advanced modeling approach was pursued."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b7004-a463-420d-83d8-bc265107d97b",
   "metadata": {},
   "source": [
    "## 3.2 XGBoost Modeling Approach\n",
    "\n",
    "Following Random Forest modeling, we implemented **XGBoost**, a more sophisticated ensemble learning method designed for high predictive performance, scalability, and better handling of imbalanced data.\n",
    "\n",
    "Preprocessing steps included:\n",
    "\n",
    "- **Preventing Data Leakage**: Post-loan variables were excluded.\n",
    "- **Handling Missing Values**:  \n",
    "  - Continuous features: imputed with the **median**  \n",
    "  - Categorical features: imputed with **\"Unknown\"**\n",
    "- **Target Encoding**: Defaults (`'Charged Off'` or `'Default'`) mapped to `1`, others to `0`.\n",
    "- **Feature Engineering**: Created `cr_hist` by computing the difference in months between the loan issue date (`issue_d`) and the borrower's earliest credit line (`earliest_cr_line`).\n",
    "- **Categorical Encoding**: One-hot encoding was used, dropping the first category to avoid multicollinearity.\n",
    "- **Type Consistency and Cleaning**: Columns were cleaned, continuous features enforced as floats, and stratified sampling was used for train-test split.\n",
    "\n",
    "\n",
    "We trained three XGBoost variants:\n",
    "\n",
    "- **Baseline XGBoost**: No class weighting, default parameters.\n",
    "- **Weighted XGBoost**: Using `scale_pos_weight` to address class imbalance.\n",
    "- **Weighted and Tuned XGBoost**: Further optimizing hyperparameters to refine model performance.\n",
    "The final selected model was the **class-weighted and tuned XGBoost**, which achieved the best trade-off between **precision**, **recall**, and **AUC**, outperforming Random Forest across all major evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855bca8-7e97-471b-b6ce-28c25e999d41",
   "metadata": {},
   "source": [
    "## 3.3 Model Validation and Final Model Selection (XGBoost)\n",
    "\n",
    "This section specifically discusses the validation and final selection of the XGBoost models.\n",
    "\n",
    "#### Overfitting Assessment  \n",
    "Model stability and generalizability were assessed by comparing training, test, and 3-fold cross-validation ROC AUC scores:\n",
    "\n",
    "| Model                      | Train AUC | Test AUC | 3-Fold CV AUC (Train)    |\n",
    "|----------------------------|-----------|----------|--------------------------|\n",
    "| Baseline XGBoost           | 0.7285    | 0.7278   | 0.7262 ± 0.0010          |\n",
    "| Tuned XGBoost (Final)      | 0.7444    | 0.7317   | 0.7295 ± 0.0011          |\n",
    "\n",
    "> Both versions show closely matched train and test scores, indicating minimal overfitting.\n",
    "\n",
    "#### Learning Curve Analysis  \n",
    "Learning curves provided insight into how performance scaled with data size:\n",
    "\n",
    "- **Training AUC** started high (~0.84) and declined to ~0.75 as more data was added.  \n",
    "- **Validation AUC** rose steadily from ~0.718 to ~0.729.  \n",
    "- The gap between training and validation narrows, showing only slight overfitting (typical for XGBoost) that remains under control.  \n",
    "\n",
    "> These patterns suggest the model generalizes reliably to new data.\n",
    "\n",
    "#### Threshold Tuning for Final XGBoost Model  \n",
    "Threshold tuning adjusts the classification cutoff (default 0.5) to favor recall over precision, aligning with our goal of minimizing undetected defaults. Performance at different thresholds:\n",
    "\n",
    "| Threshold | Precision | Recall  | F1 Score | ROC AUC |\n",
    "|-----------|-----------|---------|----------|---------|\n",
    "| 0.30      | 0.2626    | 0.9256  | 0.4091   | 0.7317  |\n",
    "| 0.50      | 0.3463    | 0.6815  | 0.4592   | 0.7317  |\n",
    "| 0.70      | 0.4886    | 0.2862  | 0.3610   | 0.7317  |\n",
    "\n",
    "At **threshold 0.30**, recall reaches **92.56%**, greatly reducing missed risky loans. Although precision drops, this trade-off is acceptable given our priority on catching defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdcd4b-3190-466a-a82f-8ff2eb77ebc4",
   "metadata": {},
   "source": [
    "## Final Model Selected\n",
    "\n",
    "The **Class-Weighted & Tuned XGBoost** with a **threshold of 0.30** was chosen as the final model. It achieved the best balance between recall, precision, and robustness while effectively minimizing the risk of missed defaults."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
